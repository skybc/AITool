"""
è®­ç»ƒè„šæœ¬ - PyTorch + YOLO11
æ”¯æŒGPU/CPUã€æ··åˆç²¾åº¦ã€æ¨¡å‹ä¿å­˜ç­‰
"""

import os
import sys
import argparse
from pathlib import Path
from datetime import datetime
import yaml
import numpy as np
import torch
import torch.nn as nn
from torch.optim import SGD, Adam
from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR
from tqdm import tqdm
import json

# ä½¿ç”¨ ultralytics YOLO11
from ultralytics import YOLO
from ultralytics.utils.metrics import box_iou

# å·¥ä¸šç¼ºé™·æ£€æµ‹è®­ç»ƒå™¨
class YoloDetector:
    """å·¥ä¸šç¼ºé™·æ£€æµ‹è®­ç»ƒå™¨"""
    
    def __init__(self, config_path: str, output_dir: str = './results'):
        """åˆå§‹åŒ–è®­ç»ƒå™¨
        
        å‚æ•°:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºæ—¥å¿—æ–‡ä»¶
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.log_file = self.output_dir / f'train_log_{timestamp}.txt'
        
        # é¦–å…ˆåŠ è½½YOLOå…¨éƒ¨é…ç½®
        all_configs_path = Path('configs/all_configs.yaml')
        if all_configs_path.exists():
            with open(all_configs_path, 'r', encoding='utf-8') as f:
                self.config = yaml.safe_load(f)
            self.log(f"âœ… å·²åŠ è½½é»˜è®¤é…ç½®: {all_configs_path}")
        else:
            self.config = {}
        
        # ç„¶ååŠ è½½ç”¨æˆ·é…ç½®ï¼Œè¦†ç›–é»˜è®¤é…ç½®
        with open(config_path, 'r', encoding='utf-8') as f:
            user_config = yaml.safe_load(f)
        
        # åˆå¹¶é…ç½®ï¼šç”¨æˆ·é…ç½®ä¼˜å…ˆ
        if user_config:
            self._merge_config(self.config, user_config)

        # ç¡®ä¿å…³é”®é…ç½®å­˜åœ¨
        self.config.setdefault('model', {})
        self.config.setdefault('training', {})
        self.task = self.config.get('task', 'detect')
        self.model_version = self.config['model'].get('version', 'yolo11')
        self.run_name = f"{self.model_version}_{self.task}"

        # ç¡®å®šè®¾å¤‡
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ç”¨äºå­˜å‚¨ trainer å¼•ç”¨çš„å˜é‡ï¼ˆä¾› GUI è½®è¯¢ä½¿ç”¨ï¼‰
        self.trainer_ref = None
        
        self.log(f"ğŸš€ å·¥ä¸šç¼ºé™·æ£€æµ‹ç³»ç»Ÿ")
        self.log(f"ğŸ“± è®¾å¤‡: {self.device}")
        self.log(f"âš™ï¸  é…ç½®æ–‡ä»¶: {config_path}\n")
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._init_model()
    
    def log(self, message: str):
        """è®°å½•æ—¥å¿—"""
        print(message)
        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(message + '\n')
    
    def _merge_config(self, base_config: dict, user_config: dict) -> None:
        """åˆå¹¶é…ç½®å­—å…¸ï¼Œç”¨æˆ·é…ç½®ä¼˜å…ˆ
        
        å‚æ•°:
            base_config: åŸºç¡€é…ç½®
            user_config: ç”¨æˆ·é…ç½®
        """
        for key, value in user_config.items():
            if key in base_config and isinstance(base_config[key], dict) and isinstance(value, dict):
                # é€’å½’åˆå¹¶åµŒå¥—å­—å…¸
                self._merge_config(base_config[key], value)
            else:
                # ç”¨æˆ·é…ç½®è¦†ç›–åŸºç¡€é…ç½®
                base_config[key] = value
    
    def _init_model(self):
        """åˆå§‹åŒ–YOLOæ¨¡å‹"""
        model_type = self.model_version  # æ¨¡å‹ç±»å‹
        backbone = self.config['model'].get('backbone', 'small')  # nano/small/medium/large/xlarge

        # æ¨¡å‹åç§°æ˜ å°„è¡¨ï¼ŒæŒ‰ä»»åŠ¡æ‹†åˆ†ï¼ˆYOLO11 æ”¯æŒ detect/segment/classify/pose/obbï¼Œå…¶å®ƒç‰ˆæœ¬ä»… detectï¼‰
        model_name_map = {
            'yolo11': {
                'detect': {
                    'nano': 'yolo11n.pt',
                    'small': 'yolo11s.pt',
                    'medium': 'yolo11m.pt',
                    'large': 'yolo11l.pt',
                    'xlarge': 'yolo11x.pt',
                },
                'segment': {
                    'nano': 'yolo11n-seg.pt',
                    'small': 'yolo11s-seg.pt',
                    'medium': 'yolo11m-seg.pt',
                    'large': 'yolo11l-seg.pt',
                    'xlarge': 'yolo11x-seg.pt',
                },
                'classify': {
                    'nano': 'yolo11n-cls.pt',
                    'small': 'yolo11s-cls.pt',
                    'medium': 'yolo11m-cls.pt',
                    'large': 'yolo11l-cls.pt',
                    'xlarge': 'yolo11x-cls.pt',
                },
                'pose': {
                    'nano': 'yolo11n-pose.pt',
                    'small': 'yolo11s-pose.pt',
                    'medium': 'yolo11m-pose.pt',
                    'large': 'yolo11l-pose.pt',
                    'xlarge': 'yolo11x-pose.pt',
                },
                'obb': {
                    'nano': 'yolo11n-obb.pt',
                    'small': 'yolo11s-obb.pt',
                    'medium': 'yolo11m-obb.pt',
                    'large': 'yolo11l-obb.pt',
                    'xlarge': 'yolo11x-obb.pt',
                },
            },
            'yolo9': {
                'detect': {
                    'nano': 'yolov9t.pt',
                    'small': 'yolov9s.pt',
                    'medium': 'yolov9m.pt',
                    'large': 'yolov9c.pt',
                    'xlarge': 'yolov9e.pt',
                }
            },
            'yolo8': {
                'detect': {
                    'nano': 'yolov8n.pt',
                    'small': 'yolov8s.pt',
                    'medium': 'yolov8m.pt',
                    'large': 'yolov8l.pt',
                    'xlarge': 'yolov8x.pt',
                }
            },
            'yolo12': {
                'detect': {
                    'nano': 'yolo12n.pt',
                    'small': 'yolo12s.pt',
                    'medium': 'yolo12m.pt',
                    'large': 'yolo12l.pt',
                    'xlarge': 'yolo12x.pt',
                }
            },
        }

        # ç‰ˆæœ¬åˆ°ç›®å½•æ˜ å°„
        version_dir_map = {
            'yolo11': '11',
            'yolo9': '9',
            'yolo8': '8',
            'yolo12': '12',
        }

        # æ£€æŸ¥ä»»åŠ¡æ”¯æŒæƒ…å†µ
        if model_type not in model_name_map:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}ã€‚æ”¯æŒçš„ç±»å‹: {list(model_name_map.keys())}")

        task_map = model_name_map[model_type]
        if self.task not in task_map:
            raise ValueError(f"{model_type} ä¸æ”¯æŒä»»åŠ¡ {self.task}ã€‚YOLO11 æ”¯æŒ detect/segment/classify/pose/obbï¼Œå…¶å®ƒä»…æ”¯æŒ detect")

        if backbone not in task_map[self.task]:
            raise ValueError(f"{model_type} ä¸æ”¯æŒ {backbone} å¤§å°ã€‚æ”¯æŒçš„å¤§å°: {list(task_map[self.task].keys())}")

        model_file_name = task_map[self.task][backbone]
        version_dir = version_dir_map.get(model_type, '11')
        
        # ä»å¯¹åº”ç‰ˆæœ¬ç›®å½•åŠ è½½é¢„è®­ç»ƒæƒé‡
        yolopt_dir = Path('yolopt') / version_dir
        model_path = yolopt_dir / model_file_name
        if not model_path.exists():
            self.log(f"âš ï¸  é¢„è®­ç»ƒæ¨¡å‹ä¸å­˜åœ¨: {model_path}")
            self.log(f"ğŸ’¡ è¯·å…ˆè¿è¡Œ: python download_models.py")
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        
       
        
        self.model = YOLO(str(model_path))
        self.model.task = self.task
        self.log(f"âœ… å·²åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {model_path} ({model_type} - {backbone}, ä»»åŠ¡: {self.task})")
    
    def _fix_data_yaml_path(self, data_yaml_path: Path, dataset_root: Path):
        """ä¿®å¤ data.yaml ä¸­çš„è·¯å¾„ï¼Œç¡®ä¿ YOLO11 èƒ½æ­£ç¡®æ‰¾åˆ°å›¾ç‰‡ 
        å‚æ•°:
            data_yaml_path: data.yaml æ–‡ä»¶è·¯å¾„
            dataset_root: æ•°æ®é›†æ ¹ç›®å½•
        """
        try:
            with open(data_yaml_path, 'r', encoding='utf-8') as f:
                data_config = yaml.safe_load(f) or {}
            
            # è·å–å½“å‰çš„è·¯å¾„é…ç½®
            current_path = data_config.get('path', '.')
            
            # å¦‚æœ path ä¸æ˜¯ç»å¯¹è·¯å¾„ï¼Œæ›´æ–°ä¸ºç»å¯¹è·¯å¾„
            path_obj = Path(current_path)
            if not path_obj.is_absolute():
                # è®¡ç®—ç›¸å¯¹äº data.yaml çš„ç»å¯¹è·¯å¾„
                abs_path = (dataset_root / current_path).resolve()
                data_config['path'] = str(abs_path)
                
                # ä¿å­˜æ›´æ–°
                with open(data_yaml_path, 'w', encoding='utf-8') as f:
                    yaml.dump(data_config, f, default_flow_style=False, allow_unicode=True)
                
                self.log(f"âœ… å·²æ›´æ–° data.yaml è·¯å¾„ä¸º: {abs_path}")
        
        except Exception as e:
            self.log(f"âš ï¸  ä¿®å¤ data.yaml è·¯å¾„å‡ºé”™: {e}")
    
    def train(self, dataset_root: str, resume: str = None):
        """è®­ç»ƒæ¨¡å‹
        
        å‚æ•°:
            dataset_root: æ•°æ®é›†æ ¹ç›®å½•
            resume: æ¢å¤è®­ç»ƒçš„æƒé‡è·¯å¾„
        """
        cfg = self.config
        task = self.task
        run_name = self.run_name
        
        # è®­ç»ƒå‚æ•°
        epochs = cfg['training']['epochs']
        batch_size = cfg['training']['batch']
        lr = cfg['training']['lr0']
        # ä½¿ç”¨å·²å‡†å¤‡å¥½çš„ data.yaml
        dataset_root = Path(dataset_root).resolve()  # è·å–ç»å¯¹è·¯å¾„
        data_yaml_path = dataset_root / 'data.yaml'
        
        # éªŒè¯æ•°æ®é›†ç»“æ„
        if not self._verify_data_yaml(dataset_root):
            return None
        
        # ä¿®å¤ data.yaml ä¸­çš„è·¯å¾„ï¼ˆç¡®ä¿ YOLO11 èƒ½æ­£ç¡®æ‰¾åˆ°å›¾ç‰‡ï¼‰
        self._fix_data_yaml_path(data_yaml_path, dataset_root)
        
        self.log(f"\nğŸ“š å¼€å§‹è®­ç»ƒ:")
        self.log(f"  - æ•°æ®é›†: {dataset_root}")
        self.log(f"  - è¿­ä»£æ•°: {epochs}")
        self.log(f"  - æ‰¹æ¬¡å¤§å°: {batch_size}")
        self.log(f"  - å­¦ä¹ ç‡: {lr}")
        self.log(f"  - ä»»åŠ¡: {task}")
            
        # ç»„è£…è®­ç»ƒå‚æ•°
        train_kwargs = {
            'data': str(data_yaml_path),
            'epochs': epochs,
            'imgsz': cfg['training']['imgsz'],
            'batch': batch_size,
            'device': 0 if self.device.type == 'cuda' else 'cpu',
            'lr0': lr,
            'lrf': 0.01,
            'momentum': 0.937,
            'weight_decay': cfg['training']['weight_decay'],
            'warmup_epochs': cfg['training']['warmup_epochs'],
            'warmup_momentum': 0.8,
            'warmup_bias_lr': 0.1,
            'patience': cfg['training']['patience'],
            'project': str(self.output_dir),
            'name': run_name,
            'exist_ok': True,
            'resume': resume is not None,
            'save': True,
            'save_period': 1,
            'seed': 42,
            'deterministic': True,
            'verbose': True,
            'amp': cfg['training']['amp'],
        }

        # æ£€æµ‹ç›¸å…³æŸå¤±åªåœ¨æ£€æµ‹ç±»ä»»åŠ¡å¯ç”¨
        if task in ['detect', 'segment', 'pose', 'obb']:
            train_kwargs.update({
                'box': cfg['training']['box'],
                'cls': cfg['training']['cls'],
                'dfl': cfg['training']['dfl'],
                'close_mosaic': cfg['training']['close_mosaic'],
            })

        results = self.model.train(**train_kwargs)
        
        # ä¿å­˜æœ€ä¼˜æ¨¡å‹åˆ°ç»“æœç›®å½•å’Œæ•°æ®ç›®å½•
        best_model_path = self.output_dir / run_name / 'weights' / 'best.pt'
        if best_model_path.exists():
            import shutil
            
            # 1. ä¿å­˜åˆ°ç»“æœç›®å½•
            final_path = self.output_dir / 'best.pt'
            shutil.copy(best_model_path, final_path)
            self.log(f"\nâœ… æœ€ä¼˜æ¨¡å‹å·²ä¿å­˜åˆ°: {final_path}")
            
            # 2. æ‹·è´åˆ°æ•°æ®é›†ç›®å½•ï¼ˆæ–¹ä¾¿ç”¨æˆ·æŸ¥æ‰¾ï¼‰
            dataset_root = Path(dataset_root).resolve()
            data_model_dir = dataset_root / 'models'
            data_model_dir.mkdir(exist_ok=True)
            
            data_model_path = data_model_dir / 'best.pt'
            shutil.copy(best_model_path, data_model_path)
            self.log(f"âœ… æœ€ä¼˜æ¨¡å‹å·²æ‹·è´åˆ°: {data_model_path}")
            
            # 3. åŒæ—¶æ‹·è´æœ€åä¸€ä¸ªepochçš„æ¨¡å‹
            last_model_path = self.output_dir / run_name / 'weights' / 'last.pt'
            if last_model_path.exists():
                last_copy_path = data_model_dir / 'last.pt'
                shutil.copy(last_model_path, last_copy_path)
                self.log(f"âœ… æœ€åæ¨¡å‹å·²æ‹·è´åˆ°: {last_copy_path}")
            
            self.log(f"\nğŸ“ æ‰€æœ‰æ¨¡å‹ä½ç½®:")
            self.log(f"  - ç»“æœç›®å½•: {self.output_dir}")
            self.log(f"  - æ•°æ®ç›®å½•: {data_model_dir}")
        
        return results
    
    def _verify_data_yaml(self, dataset_root: Path) -> bool:
        """éªŒè¯ data.yaml å’Œæ•°æ®é›†ç»“æ„
        
        è¿”å›:
            éªŒè¯æ˜¯å¦æˆåŠŸ
        """
        data_yaml_path = dataset_root / 'data.yaml'
        
        if not data_yaml_path.exists():
            self.log(f"âŒ data.yaml ä¸å­˜åœ¨: {data_yaml_path}")
            return False
        
        with open(data_yaml_path, 'r', encoding='utf-8') as f:
            data_config = yaml.safe_load(f) or {}
        
        task = self.task

        # éªŒè¯å¿…è¦çš„å­—æ®µ
        if task == 'classify':
            required_fields = ['path', 'train', 'val', 'names']
        else:
            required_fields = ['path', 'train', 'val', 'nc', 'names']

        for field in required_fields:
            if field not in data_config:
                self.log(f"âŒ data.yaml ç¼ºå°‘å­—æ®µ: {field}")
                return False

        base_path = Path(data_config.get('path', '.'))
        if not base_path.is_absolute():
            base_path = (dataset_root / base_path).resolve()

        def _resolve(p):
            p_obj = Path(p)
            return p_obj if p_obj.is_absolute() else (base_path / p_obj)

        # éªŒè¯æ•°æ®é›†ç›®å½•
        train_dir = _resolve(data_config['train'])
        val_dir = _resolve(data_config['val'])
        
        if not train_dir.exists():
            self.log(f"âŒ è®­ç»ƒæ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {train_dir}")
            return False
        
        if not val_dir.exists():
            self.log(f"âŒ éªŒè¯æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {val_dir}")
            return False
        
        self.log(f"âœ… data.yaml å’Œæ•°æ®é›†ç»“æ„éªŒè¯é€šè¿‡")
        return True

    
    def evaluate(self, dataset_root: str, weights: str = None):
        """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
        
        å‚æ•°:
            dataset_root: æ•°æ®é›†æ ¹ç›®å½•
            weights: æƒé‡æ–‡ä»¶è·¯å¾„
        """
        if weights is None:
            weights = self.output_dir / 'best.pt'
        
        if not Path(weights).exists():
            self.log(f"âŒ æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {weights}")
            return
        
        # åŠ è½½æœ€ä¼˜æ¨¡å‹
        model = YOLO(str(weights))
        
        data_yaml = Path(dataset_root) / 'data.yaml'
        
        self.log(f"\nğŸ“Š å¼€å§‹è¯„ä¼°:")
        self.log(f"  - æƒé‡: {weights}")
        self.log(f"  - æ•°æ®é›†: {data_yaml}")
        
        # è¯„ä¼°
        metrics = model.val(data=str(data_yaml), device=0 if self.device.type == 'cuda' else 'cpu')
        
        self.log(f"\nâœ… è¯„ä¼°å®Œæˆ!")
        if hasattr(metrics, 'top1'):
            self.log(f"  - top1: {metrics.top1:.4f}")
            if hasattr(metrics, 'top5'):
                self.log(f"  - top5: {metrics.top5:.4f}")
        elif hasattr(metrics, 'box'):
            self.log(f"  - mAP@0.5: {metrics.box.map50:.4f}")
            self.log(f"  - mAP@0.5:0.95: {metrics.box.map:.4f}")
        
        return metrics


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å·¥ä¸šç¼ºé™·æ£€æµ‹è®­ç»ƒè„šæœ¬')
    parser.add_argument('--config', type=str, default='config.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--dataset', type=str, required=True, help='æ•°æ®é›†æ ¹ç›®å½•')
    parser.add_argument('--output', type=str, default='./results', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--resume', type=str, default=None, help='æ¢å¤è®­ç»ƒçš„æƒé‡è·¯å¾„')
    parser.add_argument('--eval', action='store_true', help='ä»…è¯„ä¼°')
    parser.add_argument('--weights', type=str, default=None, help='è¯„ä¼°ç”¨çš„æƒé‡')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = YoloDetector(args.config, args.output)
    
    if args.eval:
        # åªè¯„ä¼°
        trainer.evaluate(args.dataset, args.weights)
    else:
        # è®­ç»ƒ
        trainer.train(args.dataset, args.resume)
        # è®­ç»ƒåè‡ªåŠ¨è¯„ä¼°
        trainer.evaluate(args.dataset)


if __name__ == '__main__':
    main()
