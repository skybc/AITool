"""
æ¨ç†è„šæœ¬ - æ”¯æŒå•å¼ /æ–‡ä»¶å¤¹æ¨ç†ã€å¯è§†åŒ–ã€ç»“æœå¯¼å‡º
æ”¯æŒ PyTorch å’Œ ONNX æ¨ç†
"""

import os
import argparse
from pathlib import Path
from typing import List, Tuple, Dict, Optional
import cv2
import numpy as np
import torch
from dataclasses import dataclass
import json
from datetime import datetime

try:
    import onnxruntime as ort
    ONNX_AVAILABLE = True
except ImportError:
    ONNX_AVAILABLE = False

from ultralytics import YOLO


@dataclass
class DetectionResult:
    """æ£€æµ‹ç»“æœæ•°æ®ç±»"""
    class_id: int
    class_name: str
    confidence: float
    bbox: Tuple[int, int, int, int]  # (x_min, y_min, x_max, y_max)
    
    def to_dict(self):
        return {
            'class_id': self.class_id,
            'class_name': self.class_name,
            'confidence': float(self.confidence),
            'bbox': list(self.bbox),
        }


class DefectInference:
    """å·¥ä¸šç¼ºé™·æ£€æµ‹æ¨ç†å™¨"""
    
    def __init__(
        self,
        weights: str,
        conf_threshold: float = 0.25,
        iou_threshold: float = 0.45,
        device: str = 'cuda',
        use_onnx: bool = False,
    ):
        """åˆå§‹åŒ–æ¨ç†å™¨
        
        å‚æ•°:
            weights: æƒé‡æ–‡ä»¶è·¯å¾„ (.pt æˆ– .onnx)
            conf_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
            iou_threshold: NMS IoUé˜ˆå€¼
            device: è®¾å¤‡ ('cuda' æˆ– 'cpu')
            use_onnx: æ˜¯å¦ä½¿ç”¨ ONNX æ¨ç†
        """
        self.weights = Path(weights)
        self.conf_threshold = conf_threshold
        self.iou_threshold = iou_threshold
        self.device = device
        
        # ç±»åˆ«ä¿¡æ¯
        self.class_names = {0: 'person'}
        self.num_classes = 1
        
        # åŠ è½½æ¨¡å‹
        if use_onnx:
            self._load_onnx_model()
        else:
            self._load_pytorch_model()
        
        print(f"âœ… æ¨¡å‹å·²åŠ è½½: {weights}")
    
    def _load_pytorch_model(self):
        """åŠ è½½ PyTorch æ¨¡å‹"""
        self.model = YOLO(str(self.weights))
        self.inference_type = 'pytorch'
    
    def _load_onnx_model(self):
        """åŠ è½½ ONNX æ¨¡å‹"""
        if not ONNX_AVAILABLE:
            raise ImportError("Please install onnxruntime: pip install onnxruntime")
        
        providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] \
            if self.device == 'cuda' else ['CPUExecutionProvider']
        
        self.ort_session = ort.InferenceSession(
            str(self.weights),
            providers=providers
        )
        
        # è·å–è¾“å…¥è¾“å‡ºä¿¡æ¯
        self.input_name = self.ort_session.get_inputs()[0].name
        self.output_names = [o.name for o in self.ort_session.get_outputs()]
        self.input_shape = self.ort_session.get_inputs()[0].shape
        
        self.inference_type = 'onnx'
        print(f"âœ… ONNX æ¨¡å‹å·²åŠ è½½ (è¾“å…¥: {self.input_shape})")
    
    def infer(self, image: np.ndarray) -> List[DetectionResult]:
        """å¯¹å•å¼ å›¾åƒè¿›è¡Œæ¨ç†
        
        å‚æ•°:
            image: è¾“å…¥å›¾åƒ (BGRæ ¼å¼, HxWx3)
            
        è¿”å›:
            æ£€æµ‹ç»“æœåˆ—è¡¨
        """
        if self.inference_type == 'pytorch':
            return self._infer_pytorch(image)
        else:
            return self._infer_onnx(image)
    
    def _infer_pytorch(self, image: np.ndarray) -> List[DetectionResult]:
        """PyTorch æ¨ç†"""
        # YOLO11 API ç›´æ¥æ¨ç†
        results = self.model(
            image,
            conf=self.conf_threshold,
            iou=self.iou_threshold,
            verbose=False
        )
        print(results[0].boxes)
        detections = []
        if results and results[0].boxes is not None:
            boxes = results[0].boxes
            
            for box in boxes:
                x_min, y_min, x_max, y_max = box.xyxy[0].cpu().numpy().astype(int)
                conf = float(box.conf[0].cpu().numpy())
                cls_id = int(box.cls[0].cpu().numpy())
                
                detections.append(DetectionResult(
                    class_id=cls_id,
                    class_name=self.class_names.get(cls_id, 'unknown'),
                    confidence=conf,
                    bbox=(x_min, y_min, x_max, y_max),
                ))
        
        return detections
    
    def _infer_onnx(self, image: np.ndarray) -> List[DetectionResult]:
        """ONNX æ¨ç†"""
        # é¢„å¤„ç†
        img_resized = cv2.resize(image, (640, 640))
        img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)
        img_normalized = img_rgb.astype(np.float32) / 255.0
        img_transposed = np.transpose(img_normalized, (2, 0, 1))
        img_batch = np.expand_dims(img_transposed, axis=0)
        
        # æ¨ç†
        outputs = self.ort_session.run(self.output_names, {self.input_name: img_batch})
        
        # åå¤„ç† (YOLO11 ONNX è¾“å‡ºæ ¼å¼)
        predictions = outputs[0][0]  # (num_detections, 6)
        
        # æ¢å¤åˆ°åŸå§‹å›¾åƒåæ ‡
        h_orig, w_orig = image.shape[:2]
        h_resized, w_resized = 640, 640
        scale_x = w_orig / w_resized
        scale_y = h_orig / h_resized
        
        detections = []
        for pred in predictions:
            x_center, y_center, width, height, conf = pred[:5]
            cls_id = int(pred[5])
            
            if conf < self.conf_threshold:
                continue
            
            # ä»ä¸­å¿ƒåæ ‡è½¬æ¢ä¸ºè§’åæ ‡
            x_min = int((x_center - width / 2) * scale_x)
            y_min = int((y_center - height / 2) * scale_y)
            x_max = int((x_center + width / 2) * scale_x)
            y_max = int((y_center + height / 2) * scale_y)
            
            # è¾¹ç•Œçº¦æŸ
            x_min = max(0, x_min)
            y_min = max(0, y_min)
            x_max = min(w_orig, x_max)
            y_max = min(h_orig, y_max)
            
            detections.append(DetectionResult(
                class_id=cls_id,
                class_name=self.class_names.get(cls_id, 'unknown'),
                confidence=float(conf),
                bbox=(x_min, y_min, x_max, y_max),
            ))
        
        return detections
    
    def infer_batch(self, image_paths: List[str]) -> Dict[str, List[DetectionResult]]:
        """æ‰¹é‡æ¨ç†
        
        å‚æ•°:
            image_paths: å›¾åƒè·¯å¾„åˆ—è¡¨
            
        è¿”å›:
            {å›¾åƒè·¯å¾„: æ£€æµ‹ç»“æœ}
        """
        results = {}
        for img_path in image_paths:
            image = cv2.imread(img_path)
            if image is None:
                print(f"âš ï¸  æ— æ³•è¯»å–å›¾åƒ: {img_path}")
                continue
            
            detections = self.infer(image)
            results[img_path] = detections
        
        return results
    
    def visualize(
        self,
        image: np.ndarray,
        detections: List[DetectionResult],
        save_path: Optional[str] = None,
    ) -> np.ndarray:
        """å¯è§†åŒ–æ£€æµ‹ç»“æœ
        
        å‚æ•°:
            image: åŸå§‹å›¾åƒ
            detections: æ£€æµ‹ç»“æœåˆ—è¡¨
            save_path: ä¿å­˜è·¯å¾„ (å¯é€‰)
            
        è¿”å›:
            å¯è§†åŒ–åçš„å›¾åƒ
        """
        vis_image = image.copy()
        
        # é¢œè‰²
        color = (0, 255, 0)  # ç»¿è‰² (BGR)
        thickness = 2
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.6
        font_thickness = 1
        
        for det in detections:
            x_min, y_min, x_max, y_max = det.bbox
            
            # ç»˜åˆ¶è¾¹æ¡†
            cv2.rectangle(vis_image, (x_min, y_min), (x_max, y_max), color, thickness)
            
            # ç»˜åˆ¶æ ‡ç­¾
            label = f"{det.class_name} {det.confidence:.2f}"
            text_size = cv2.getTextSize(label, font, font_scale, font_thickness)[0]
            text_x = x_min
            text_y = y_min - 5
            
            # ç»˜åˆ¶æ–‡å­—èƒŒæ™¯
            cv2.rectangle(
                vis_image,
                (text_x, text_y - text_size[1] - 4),
                (text_x + text_size[0], text_y),
                color,
                -1
            )
            
            # ç»˜åˆ¶æ–‡å­—
            cv2.putText(
                vis_image,
                label,
                (text_x, text_y - 2),
                font,
                font_scale,
                (255, 255, 255),  # ç™½è‰²æ–‡å­—
                font_thickness
            )
        
        # ä¿å­˜
        if save_path:
            cv2.imwrite(save_path, vis_image)
            print(f"âœ… å¯è§†åŒ–ç»“æœå·²ä¿å­˜: {save_path}")
        
        return vis_image
    
    def export_results(self, results: Dict, save_path: str):
        """å¯¼å‡ºæ¨ç†ç»“æœä¸º JSON
        
        å‚æ•°:
            results: {å›¾åƒè·¯å¾„: æ£€æµ‹ç»“æœ}
            save_path: ä¿å­˜è·¯å¾„
        """
        export_data = {
            'timestamp': datetime.now().isoformat(),
            'results': {}
        }
        
        for img_path, detections in results.items():
            export_data['results'][img_path] = [det.to_dict() for det in detections]
        
        with open(save_path, 'w') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ç»“æœå·²å¯¼å‡º: {save_path}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ç¼ºé™·æ£€æµ‹æ¨ç†è„šæœ¬')
    parser.add_argument('--weights', type=str, required=True, help='æƒé‡æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--image', type=str, default=None, help='å•å¼ å›¾åƒè·¯å¾„')
    parser.add_argument('--folder', type=str, default=None, help='å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„')
    parser.add_argument('--conf', type=float, default=0.25, help='ç½®ä¿¡åº¦é˜ˆå€¼')
    parser.add_argument('--iou', type=float, default=0.45, help='NMS IoUé˜ˆå€¼')
    parser.add_argument('--device', type=str, default='cuda', help='è®¾å¤‡ (cuda/cpu)')
    parser.add_argument('--onnx', action='store_true', help='ä½¿ç”¨ ONNX æ¨ç†')
    parser.add_argument('--save-vis', type=str, default=None, help='ä¿å­˜å¯è§†åŒ–ç›®å½•')
    parser.add_argument('--save-json', type=str, default=None, help='ä¿å­˜JSONç»“æœæ–‡ä»¶')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ¨ç†å™¨
    inferencer = DefectInference(
        weights=args.weights,
        conf_threshold=args.conf,
        iou_threshold=args.iou,
        device=args.device,
        use_onnx=args.onnx,
    )
    
    # å‡†å¤‡è¾“å‡ºç›®å½•
    if args.save_vis:
        Path(args.save_vis).mkdir(parents=True, exist_ok=True)
    
    # æ¨ç†
    if args.image:
        # å•å¼ å›¾åƒ
        print(f"\nğŸ–¼ï¸  æ¨ç†å•å¼ å›¾åƒ: {args.image}")
        image = cv2.imread(args.image)
        if image is None:
            print(f"âŒ æ— æ³•è¯»å–å›¾åƒ: {args.image}")
            return
        
        detections = inferencer.infer(image)
        
        print(f"âœ… æ£€æµ‹åˆ° {len(detections)} ä¸ªç›®æ ‡:")
        for det in detections:
            print(f"  - {det.class_name}: {det.confidence:.2f} {det.bbox}")
        
        # å¯è§†åŒ–
        if args.save_vis:
            vis_image = inferencer.visualize(
                image,
                detections,
                os.path.join(args.save_vis, Path(args.image).stem + '_vis.jpg')
            )
    
    elif args.folder:
        # æ–‡ä»¶å¤¹æ¨ç†
        print(f"\nğŸ“ æ¨ç†æ–‡ä»¶å¤¹: {args.folder}")
        image_dir = Path(args.folder)
        image_files = sorted(
            list(image_dir.glob('*.jpg')) +
            list(image_dir.glob('*.png')) +
            list(image_dir.glob('*.bmp'))
        )
        
        print(f"ğŸ“Š æ‰¾åˆ° {len(image_files)} å¼ å›¾åƒ")
        
        all_results = {}
        for img_path in image_files:
            image = cv2.imread(str(img_path))
            if image is None:
                continue
            
            detections = inferencer.infer(image)
            all_results[str(img_path)] = detections
            
            print(f"âœ… {img_path.name}: {len(detections)} ä¸ªç›®æ ‡")
            
            # å¯è§†åŒ–
            if args.save_vis:
                vis_image = inferencer.visualize(
                    image,
                    detections,
                    os.path.join(args.save_vis, img_path.stem + '_vis.jpg')
                )
        
        # å¯¼å‡ºç»“æœ
        if args.save_json:
            inferencer.export_results(all_results, args.save_json)
    
    else:
        print("âŒ è¯·æŒ‡å®š --image æˆ– --folder")


if __name__ == '__main__':
    main()
